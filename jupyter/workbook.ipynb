{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COIN Project\n",
    "\n",
    "Authors:\n",
    "\n",
    "    - Mona\n",
    "    - Roman\n",
    "    - Nick\n",
    "    - Mateo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sa\n",
    "import tweepy\n",
    "#import dotenv\n",
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../twitter.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accounts': ['Fridays4future',\n",
       "  'FridayForFuture',\n",
       "  'GretaThunberg',\n",
       "  'Luisamneubauer',\n",
       "  'AnnaUKSCN',\n",
       "  'AnunaDe',\n",
       "  'adelaidecharli2',\n",
       "  'HollyWildChild',\n",
       "  'AlexandriaV2005',\n",
       "  'fff_europe',\n",
       "  'FFFUnitedStates',\n",
       "  'fff_digital'],\n",
       " 'hashtags': {'green': ['EndCoal',\n",
       "   'EndFossilFuels',\n",
       "   'PeopleNotProfit',\n",
       "   'NoMoreEmptyPromises',\n",
       "   'UprootTheSystem',\n",
       "   'FridaysForFuture',\n",
       "   'ClimateAction',\n",
       "   'ClimateJustice',\n",
       "   'ClimateEmergency',\n",
       "   'ClimateStrike',\n",
       "   'SaveThePlanet'],\n",
       "  'brown': ['climatescam',\n",
       "   'climatechangehoax',\n",
       "   'fakeclimate',\n",
       "   'climatehoax',\n",
       "   'globalwarmingisahoax'],\n",
       "  'neutral': ['ClimateCrisis', 'ClimateChange', 'Climate', 'GlobalWarming']}}"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Research Access:\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAGjtbgEAAAAAyx9oZIai1hXZ9OyDhUUFMZQivpc%3DUunewUXR9hw3nyKQhjqdmfg7zSAoa1nPv6WKLLSPB7OwKwYBP3'\n",
    "\n",
    "#Initializing the client to request the Twitter API\n",
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)\n",
    "\n",
    "#expansions = ['attachments.poll_ids', 'attachments.media_keys', 'author_id', 'entities.mentions.username', 'geo.place_id', 'in_reply_to_user_id', 'referenced_tweets.id', 'referenced_tweets.id.author_id']\n",
    "expansions = ['attachments.media_keys','author_id','geo.place_id','referenced_tweets.id','referenced_tweets.id.author_id']\n",
    "max_results = 500\n",
    "media_fields = [\"duration_ms\", \"height\", \"media_key\", \"preview_image_url\", \"type\", \"url\", \"width\", \"public_metrics\", \"non_public_metrics\", \"organic_metrics\", \"promoted_metrics\", \"alt_text\"]\n",
    "#next_token = ''#(str | None) – This parameter is used to get the next ‘page’ of results. The value used with the parameter is pulled directly from the response provided by the API, and should not be modified. You can learn more by visiting our page on pagination.\n",
    "place_fields = ['country', 'country_code', 'full_name', 'geo', 'id', 'name', 'place_type']\n",
    "#poll_fields #(list[str] | str | None) – poll_fields\n",
    "#since_id #(int | str | None) – Returns results with a Tweet ID greater than (for example, more recent than) the specified ID. The ID specified is exclusive and responses will not include it. If included with the same request as a start_time parameter, only since_id will be used.\n",
    "sort_order = 'recency'\n",
    "start_time = '2017-01-01T00:00:00Z' #(datetime.datetime | str | None) – YYYY-MM-DDTHH:mm:ssZ (ISO 8601/RFC 3339). The oldest UTC timestamp from which the Tweets will be provided. Timestamp is in second granularity and is inclusive (for example, 12:00:01 includes the first second of the minute). By default, a request will return Tweets from up to 30 days ago if you do not include this parameter.\n",
    "#end_time = '2019-07-15T23:00:00Z'\n",
    "#tweet_fields = [\"attachments\", \"author_id\", \"context_annotations\", \"conversation_id\", \"created_at\", \"entities\", \"geo\", \"id\", \"in_reply_to_user_id\", \"lang\", \"non_public_metrics\", \"public_metrics\", \"organic_metrics\", \"promoted_metrics\", \"possibly_sensitive\", \"referenced_tweets\", \"reply_settings\", \"source\", \"text\", \"withheld\"]\n",
    "tweet_fields = [\"attachments\",\"author_id\", \"conversation_id\",\"created_at\",\"referenced_tweets\",\"geo\",\"public_metrics\"]\n",
    "#until_id #(int | str | None) – Returns results with a Tweet ID less than (that is, older than) the specified ID. Used with since_id. The ID specified is exclusive and responses will not include it.\n",
    "#user_fields can be extracted from the .includes attribute of the tweepy response\n",
    "user_fields = [\"created_at\", \"description\", \"location\", \"name\", \"pinned_tweet_id\", \"profile_image_url\", \"protected\", \"public_metrics\", \"url\", \"username\", \"verified\", \"withheld\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Load and Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify tweets to look up\n",
    "'''query_list = ['EndCoal', 'EndFossilFuels', 'PeopleNotProfit', 'NoMoreEmptyPromises', 'UprootTheSystem', \n",
    "              'FridaysForFuture', 'ClimateAction', 'ClimateJustice', 'ClimateEmergency', 'ClimateStrike', \n",
    "              'SaveThePlanet', 'climatescam', 'climatechangehoax', 'fakeclimate', 'climatehoax', \n",
    "              'globalwarmingisahoax', 'ClimateCrisis', 'ClimateChange', 'Climate', 'GlobalWarming']'''\n",
    "\n",
    "query_list = ['FridaysForFuture', 'ClimateAction', 'ClimateJustice', 'ClimateEmergency', 'ClimateStrike']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_tweets(query):\n",
    "    response = client.search_all_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=max_results\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              ,start_time=start_time\n",
    "                              #,end_time=end_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_tweets_with_next_token(query, next_token):\n",
    "    response = client.search_all_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=max_results\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              ,start_time=start_time\n",
    "                              #,end_time=end_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                              ,next_token=next_token\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_data(query, iteration_count, next_token):\n",
    "\n",
    "    \"\"\" Fetches iteration_count * 100 Tweets from Twitter API using the set query params, including the initial next_token\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    * iteration_count     : Integer indicating the max number of requests \n",
    "    * next_token          : String containing the next token for pagination\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the return parameters\n",
    "    tweets = []\n",
    "    users = []\n",
    "    ref_tweets = []\n",
    "    media = []\n",
    "    places = []\n",
    "    \n",
    "    for i in range(0, iteration_count):\n",
    "\n",
    "        if(len(next_token) == 0):\n",
    "            response = search_all_tweets(query)\n",
    "        else:\n",
    "            response = search_all_tweets_with_next_token(query, next_token)\n",
    "\n",
    "        tweets.append(response.data)\n",
    "        users.append(response.includes['users'])\n",
    "        ref_tweets.append(response.includes['tweets'])\n",
    "        if \"media\" in response.includes.keys():\n",
    "            media.append(response.includes['media'])\n",
    "        \n",
    "        if('places' in response.includes.keys()):\n",
    "            places.append(response.includes['places'])\n",
    "            \n",
    "        try:\n",
    "            next_token = response.meta['next_token']\n",
    "        except:\n",
    "            print('No more Tweets to fetch')\n",
    "            break\n",
    "\n",
    "\n",
    "    # Logging the last next_token for pagination into console --> needed for later requesting of the next tweets\n",
    "    if('next_token' in response.meta):\n",
    "        print(response.meta['next_token'])\n",
    "        \n",
    "    return tweets, users, ref_tweets, media, places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dict(unpack_dict):\n",
    "    unpacked = [v for k, v in unpack_dict.items()]\n",
    "    return unpacked\n",
    "def unpack_attachment(attachment_dict):\n",
    "    return \"|\".join(attachment_dict[\"media_keys\"]) if \"media_keys\" in attachment_dict.keys() else np.nan\n",
    "def unpack_geo(geo_dict):\n",
    "    return geo_dict[\"type\"], \"|\".join([str(x) for x in geo_dict[\"bbox\"]])\n",
    "def unpack_geo_2(geo_dict):\n",
    "    return geo_dict[\"place_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tweets(query):    \n",
    "    tweets, users, ref_tweets, media, places = fetch_all_data(query, 300, '')\n",
    "\n",
    "    user_df = pd.DataFrame(data=[user for sublist in users for user in sublist])\n",
    "    tweet_df = pd.DataFrame(data=[tweet for sublist in tweets for tweet in sublist])\n",
    "    ref_tweet_df = pd.DataFrame(data=[tweet for sublist in ref_tweets for tweet in sublist])\n",
    "    media_df = pd.DataFrame(data=[item for sublist in media for item in sublist])\n",
    "    place_df = pd.DataFrame(data=[place for sublist in places for place in sublist])\n",
    "    \n",
    "    user_df[[\"followers_count\", \"following_count\", \"tweet_count\", \"listed_count\"]] = user_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    del user_df[\"public_metrics\"]\n",
    "\n",
    "    tweet_df[[\"retweet_count\", \"reply_count\", \"like_count\", \"quote_count\"]] = tweet_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    tweet_df[\"media_keys\"] = tweet_df.loc[tweet_df[\"attachments\"].notna()].apply(lambda x: unpack_attachment(x[\"attachments\"]), axis=1, result_type=\"expand\")\n",
    "    del tweet_df[\"public_metrics\"]\n",
    "    del tweet_df[\"attachments\"]\n",
    "    del tweet_df[\"referenced_tweets\"]\n",
    "\n",
    "    ref_tweet_df[['retweet_count', 'reply_count', 'like_count', 'quote_count']] = ref_tweet_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    ref_tweet_df[\"media_keys\"] = ref_tweet_df.loc[ref_tweet_df[\"attachments\"].notna()].apply(lambda x: unpack_attachment(x[\"attachments\"]), axis=1, result_type=\"expand\")\n",
    "    ref_tweet_df['place_id'] = ref_tweet_df.loc[ref_tweet_df[\"geo\"].notna()].apply(lambda x: unpack_geo_2(x[\"geo\"]), axis=1, result_type=\"expand\")\n",
    "    del ref_tweet_df[\"public_metrics\"]\n",
    "    del ref_tweet_df[\"attachments\"]\n",
    "    del ref_tweet_df[\"geo\"]\n",
    "    del ref_tweet_df[\"referenced_tweets\"]\n",
    "\n",
    "    if \"geo\" in place_df.columns:\n",
    "        place_df[[\"type\", \"coordinates\"]] = place_df.loc[place_df[\"geo\"].notna()].apply(lambda x: unpack_geo(x[\"geo\"]), axis=1, result_type=\"expand\")\n",
    "        del place_df[\"geo\"]\n",
    "        \n",
    "    tweet_df.text = tweet_df.text.apply(lambda x: re.sub(r\"[^A-Za-z0-9\\w\\s:@,]\", \"\", x))\n",
    "    return user_df, tweet_df, ref_tweet_df, place_df, media_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FridaysForFuture\n",
      "b26v89c19zqg8o3fpyqo6y2lfez3zzfj2zvwqlp0f4519\n",
      "ClimateAction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 169 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b26v89c19zqg8o3fpywln07eeiuh5g5ony38yzbqeeh6l\n",
      "ClimateJustice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 119 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b26v89c19zqg8o3fpe45y984n3fd66eshj1iqq5819bp9\n",
      "ClimateEmergency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 142 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b26v89c19zqg8o3fpytot5byqjn9v9dejjg68s0gq0vb1\n",
      "ClimateStrike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 33 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b26v89c19zqg8o3fpe18ak60yd03dlyi3wbea5gs7ykql\n"
     ]
    }
   ],
   "source": [
    "all_user = all_tweets = all_ref_tweets = all_places = all_media = pd.DataFrame()\n",
    "for i in query_list:\n",
    "    print(i)\n",
    "    user_df, tweet_df, ref_tweet_df, place_df, media_df = query_tweets(i)\n",
    "    all_user = pd.concat([all_user, user_df])\n",
    "    all_tweets = pd.concat([all_user, tweet_df])\n",
    "    all_ref_tweets = pd.concat([all_user, ref_tweet_df])\n",
    "    all_places = pd.concat([all_user, place_df])\n",
    "    all_media = pd.concat([all_user, media_df])\n",
    "    \n",
    "    #time.sleep(60)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets fetched (rows, columns): (148492, 25)\n"
     ]
    }
   ],
   "source": [
    "all_tweets.dropna(subset=['text'], inplace=True)\n",
    "print('Number of tweets fetched (rows, columns):', all_tweets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148492, 12)\n",
      "(664636, 16)\n"
     ]
    }
   ],
   "source": [
    "#drop emtpy tweet columns\n",
    "all_tweets.drop(['description', 'location', 'name', 'pinned_tweet_id', 'profile_image_url', 'protected',\n",
    "                 'url', 'username', 'verified', 'followers_count', 'following_count','tweet_count',\n",
    "                'listed_count'], axis=1, inplace=True)\n",
    "print(all_tweets.shape)\n",
    "print(all_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust naming and datatypes\n",
    "all_tweets['author_id'] = [int(i) for i in all_tweets['author_id']]\n",
    "all_user.rename(columns={\"id\": \"author_id\"}, inplace=True)\n",
    "all_user.rename(columns={\"created_at\": \"user_created_at\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202962, 7)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop duplicate users\n",
    "all_user_unique = all_user.groupby('author_id').mean()\n",
    "all_user_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106852, 19)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put tweet and user info in one file\n",
    "twitter_dataset = pd.merge(all_tweets,all_user_unique, on='author_id')\n",
    "twitter_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "twitter_dataset.to_csv(\"twitter_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframe to csv\n",
    "#all_user.to_csv(\"tweets_by_user.csv\", index=False)\n",
    "#all_tweets.to_csv(\"tweets_by_hashtags.csv\", index=False)\n",
    "#all_ref_tweets.to_csv(\"tweets_reftweets.csv\", index=False, header=False)\n",
    "#all_places.to_csv(\"tweets_places.csv\", index=False)\n",
    "#all_media.to_csv(\"tweets_media.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd10460dd3d146ddcd00478931b69fe3eeab4952c1785708c7385e1d4193c119"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
