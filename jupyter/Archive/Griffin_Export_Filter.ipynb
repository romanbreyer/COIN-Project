{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns dataframe consisting of the separate files in specified directory\n",
    "\n",
    "def import_data_from_folder(path):\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.json\"))\n",
    "    frames = []\n",
    "    \n",
    "    # loop over the list of files\n",
    "    for f in csv_files:\n",
    "\n",
    "        # read the csv file\n",
    "        df = pd.read_json(f)\n",
    "        frames.append(df)\n",
    "    \n",
    "    # Concat DataFrame list to single tweet_dfFrame\n",
    "    res_df = pd.concat(frames)\n",
    "    res_df.reset_index(inplace=True)\n",
    "\n",
    "    # Drop Index column which is also created while importing files\n",
    "    res_df.drop(columns=['index'], inplace=True)\n",
    "    \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns a list containing all hashtags in the given list of JSON objects\n",
    "\n",
    "def extract_hashtags(hashtag_ls:list):\n",
    "    \n",
    "    hashtags = [item.get('tag','') for item in hashtag_ls]\n",
    "    \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function cleans twitter_query string \n",
    "\n",
    "def clean_twitter_query(query:str):\n",
    "    \n",
    "    query = re.sub('from:','',query)\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DEPRECATED\n",
    "# # Calculating the Nodes for the network\n",
    "# def get_griffin_nodes(tweet_df:pd.DataFrame):\n",
    "\n",
    "#     nodes_ls = []\n",
    "\n",
    "#     for index, user in data.iterrows():\n",
    "#         #print(user.get('public_metrics').get())\n",
    "#         node = {}\n",
    "        \n",
    "#         # Author_id is only used for later joining\n",
    "#         node['author_id'] =  user.get('id','')\n",
    "#         node['id'] = user.get('username','')\n",
    "#         node['name'] = user.get('name','')\n",
    "\n",
    "#         # Favourites Counts is not provided by Twitter API V2\n",
    "#         node['favourites_count'] = 0 # TODO muss noch ergänzt werden, wird von der Twitter API nicht zurückgegeben\n",
    "#         node['followers_count'] = user.get(\"public_metrics\",{}).get('followers_count','')\n",
    "#         node['friends_count'] =  user.get(\"public_metrics\",{}).get('following_count','')\n",
    "#         node['listed_count']= user.get(\"public_metrics\",{}).get('listed_count','')\n",
    "#         node['statuses_count'] = user.get(\"public_metrics\",{}).get('tweet_count','')\n",
    "#         node['twitter_query'] = re.sub('from:','',user.get(\"twitter_query\",''))\n",
    "\n",
    "#         nodes_ls.append(node)\n",
    "\n",
    "#     nodes_df = pd.DataFrame(nodes_ls)\n",
    "    \n",
    "#     return nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Nodes for the network\n",
    "def get_griffin_nodes(data:pd.DataFrame):\n",
    "\n",
    "    nodes_df =  data[[\n",
    "                    'id',\n",
    "                    'username',\n",
    "                    'name',\n",
    "                    'public_metrics',\n",
    "                    'twitter_query'\n",
    "                    ]]\n",
    "    \n",
    "    # Cleaning up 'twitter_query' column\n",
    "    nodes_df['twitter_query'] = nodes_df['twitter_query'].apply(lambda query: clean_twitter_query(query))\n",
    "\n",
    "    # Extracting public_metrics out of the user dataframe\n",
    "    public_metrics_df = pd.json_normalize(nodes_df['public_metrics'])\n",
    "    nodes_df = nodes_df.merge(public_metrics_df, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "    # Adding favourites_count\n",
    "    nodes_df['favourites_count'] = 0\n",
    "\n",
    "    # Renaming the columns\n",
    "    nodes_df.rename(columns={\n",
    "        'id':'author_id',\n",
    "        'username':'id',\n",
    "        'following_count':'friends_count',\n",
    "        'tweet_count':'statuses_count',\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Removing duplicates\n",
    "    # Assumption: Ordering by statuses (total tweet) count will deliver a list of users where the most recent data of the user are in \n",
    "    # the last record (ascending order)\n",
    "    nodes_df.sort_values(by=['author_id','statuses_count'], inplace=True, ascending=True)\n",
    "    nodes_df.drop_duplicates(subset='author_id', inplace=True, keep='last')\n",
    "    nodes_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Reordering the columns\n",
    "    nodes_df = nodes_df[[\n",
    "        'author_id',\n",
    "        'id',\n",
    "        'name',\n",
    "        'favourites_count',\n",
    "        'followers_count',\n",
    "        'friends_count',\n",
    "        'listed_count',\n",
    "        'statuses_count',\n",
    "        'twitter_query'\n",
    "    ]]\n",
    "    \n",
    "    return nodes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nodes(df):\n",
    "    #clean name from unwanted characters\n",
    "    df['name'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "    df['name'] = df['name'].replace({'\"': '' }, regex=True)\n",
    "    df['name'] = df['name'].replace({'\\'': '' }, regex=True)\n",
    "    df['name'] = df['name'].replace({' &amp;': '' }, regex=True)\n",
    "    df['name'] = df['name'].replace({';': ',' }, regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_edges(df):\n",
    "    #clean twitter text form unwanted characters\n",
    "    df['text'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "    df['text'] = df['text'].replace({'\"': '' }, regex=True)\n",
    "    df['text'] = df['text'].replace({';': ',' }, regex=True)\n",
    "    df['text'] = df['text'].replace({' &amp;': '' }, regex=True)\n",
    "    df['text'] = df['text'].replace({'&amp;': '' }, regex=True)\n",
    "    #clean source- and destination-name from unwanted characters\n",
    "    df['src_name'] = df['src_name'].replace({'\\'': '' }, regex=True)\n",
    "    df['dst_name'] = df['dst_name'].replace({'\\'': '' }, regex=True)\n",
    "    df['src_name'] = df['src_name'].replace({';': ',' }, regex=True)\n",
    "    df['dst_name'] = df['dst_name'].replace({';': ',' }, regex=True)\n",
    "    #format the datetime to be griffin readable\n",
    "    df['created_at'] = pd.to_datetime(df[\"created_at\"], format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c81a55b7875b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing all files into dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtweet_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_data_from_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Data/Tweets/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0muser_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimport_data_from_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Data/Users/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#ref_tweet_df = import_data_from_folder(\"../Data/Retweets/\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-f1bd6e4d78a4>\u001b[0m in \u001b[0;36mimport_data_from_folder\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Concat DataFrame list to single tweet_dfFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mres_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mres_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\monaw\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \"\"\"\n\u001b[1;32m--> 285\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\monaw\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# Importing all files into dataframes\n",
    "\n",
    "tweet_df = import_data_from_folder(\"../Data/Tweets/\")\n",
    "user_df = import_data_from_folder(\"../Data/Users/\")\n",
    "#ref_tweet_df = import_data_from_folder(\"../Data/Retweets/\")\n",
    "#media_df = import_data_from_folder(\"../Data/Media/\")\n",
    "#place_df = import_data_from_folder(\"../Data/Place/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Griffin Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = get_griffin_nodes(user_df)\n",
    "#nodes_df.drop(columns='author_id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Data Cleaning (for calculating Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Tweet type and discard all retweets --> For constructing the network, retweets only lead to a high degree\n",
    "# Computationally the calculation of the network is still very very costly\n",
    "tweet_df = tweet_df.explode(column='referenced_tweets')\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retweet(ref_tweet:dict):\n",
    "    is_retweet = False\n",
    "\n",
    "    if(ref_tweet):\n",
    "        if(ref_tweet.get('type') == 'retweeted'):\n",
    "            is_retweet = True\n",
    "        \n",
    "    return is_retweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['retweet'] = tweet_df['referenced_tweets'].apply(lambda ref_tweet: is_retweet(ref_tweet))\n",
    "#tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all (actual) Retweets from tweet_df\n",
    "# Data set then contains Replies, Quoted Tweets (commented Retweets) and regular Tweets\n",
    "tweet_cleaned_df = tweet_df[tweet_df['retweet'] == False]\n",
    "#tweet_cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of Griffin Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there are some Tweets without entities (mentions AND hashtags). Those can be dropped, because they would be displayed as single points in the network or as reference to the twitter search query.\n",
    "\n",
    "Note: We also filtered out / do not calculate the connections between nodes and the twitter_search_query. These would be filtered out in the first steps in Griffin anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Tweets without any entities / mentions\n",
    "tweet_cleaned_df.dropna(subset=['entities'], inplace=True)\n",
    "len(tweet_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate tweets (based on same author_id and text)\n",
    "tweet_cleaned_df.drop_duplicates(subset=['author_id', 'text'], inplace=True, keep='first')\n",
    "tweet_cleaned_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Mentions and Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mentions and hashtags list from entities\n",
    "tweet_cleaned_df['mentions'] = tweet_cleaned_df['entities'].apply(lambda entity: entity.get('mentions'))\n",
    "tweet_cleaned_df['hashtags_ls'] = tweet_cleaned_df['entities'].apply(lambda entity: entity.get('hashtags'))\n",
    "tweet_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all rows which don't contain any mentions, because they will appear as points in the network\n",
    "\n",
    "tweet_cleaned_df.dropna(axis=0, subset='mentions', inplace=True)\n",
    "len(tweet_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand all mentions in dataframe\n",
    "tweet_cleaned_df = tweet_cleaned_df.explode(column='mentions')\n",
    "tweet_cleaned_df.reset_index(drop=True, inplace=True)\n",
    "len(tweet_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract username and ids of mentioned users into Griffin edge attributes dst, dst_screen_name, dst_id_str\n",
    "tweet_cleaned_df['dst'] = tweet_cleaned_df['mentions'].apply(lambda mention: mention.get('username',''))\n",
    "tweet_cleaned_df['dst_screen_name'] = tweet_cleaned_df['mentions'].apply(lambda mention: mention.get('username',''))\n",
    "tweet_cleaned_df['dst_id_str'] = tweet_cleaned_df['mentions'].apply(lambda mention: mention.get('id',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hashtags from tweets into list for each Tweet\n",
    "tweet_cleaned_df['hashtags'] = tweet_cleaned_df[tweet_cleaned_df['hashtags_ls'].notna()]['hashtags_ls'].apply(lambda ls: extract_hashtags(ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping irrelevant columns (for Griffin Edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns for Griffin\n",
    "edges_df = tweet_cleaned_df.drop(columns=['entities',\n",
    "               'id',\n",
    "               'referenced_tweets',\n",
    "               'lang',\n",
    "               'public_metrics',\n",
    "               'conversation_id',\n",
    "               'in_reply_to_user_id',\n",
    "               'attachments',\n",
    "               'geo',\n",
    "               'withheld',\n",
    "               'mentions',\n",
    "               'hashtags_ls',\n",
    "               'retweet'], axis=1)\n",
    "\n",
    "edges_df.rename(columns={'author_id':'src_id_str'}, inplace=True)\n",
    "edges_df['dst_id_str'] = edges_df['dst_id_str'].astype(int)\n",
    "#edges_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Nodes (src and dst) to create griffin edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe containing the required fields for SRC nodes\n",
    "src = nodes_df[['author_id',\n",
    "               'id',\n",
    "               'name',\n",
    "               'favourites_count',\n",
    "               'followers_count',\n",
    "               'friends_count',\n",
    "               'listed_count',\n",
    "               'statuses_count']].copy()\n",
    "\n",
    "src.rename(columns={\n",
    "   'author_id':'src_id_str',\n",
    "   'id':'src_screen_name',\n",
    "   'name':'src_name',\n",
    "   'favourites_count':'src_favourites_count',\n",
    "   'followers_count':'src_followers_count',\n",
    "   'friends_count':'src_friends_count',\n",
    "   'listed_count':'src_listed_count',\n",
    "   'statuses_count':'src_statuses_count'\n",
    "    }, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes to create SRC columns\n",
    "\n",
    "edges_src = edges_df.merge(src, how='left', left_on='src_id_str', right_on='src_id_str')\n",
    "edges_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe containing the required fields for DST nodes\n",
    "dst = nodes_df[['author_id',\n",
    "               'name'\n",
    "               ]].copy()\n",
    "\n",
    "dst.rename(columns={\n",
    "   'author_id':'dst_id_str',\n",
    "   'name':'dst_name'\n",
    "    }, inplace=True)\n",
    "\n",
    "dst['dst_id_str'] = dst['dst_id_str'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes to create DST columns\n",
    "\n",
    "edges_df = edges_src.merge(dst, how='inner', left_on='dst_id_str', right_on='dst_id_str')\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Unix Time Stamp Column\n",
    "edges_df['time'] = edges_df['created_at'].apply(lambda x: int(time.mktime(x.timetuple())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src column\n",
    "edges_df['src'] = edges_df['src_screen_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder Columns for Griffin Format\n",
    "edges_df[['src_favourites_count',\n",
    "'src_followers_count',\n",
    "'src_friends_count',\n",
    "'src_listed_count',\n",
    "'src_statuses_count',\n",
    "'src_name',\n",
    "'src_screen_name',\n",
    "'src_id_str',\n",
    "'text',\n",
    "'created_at',\n",
    "'twitter_query',\n",
    "'hashtags',\n",
    "'dst_name',\n",
    "'dst_screen_name',\n",
    "'dst_id_str',\n",
    "'time',\n",
    "'src',\n",
    "'dst']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Edges and Nodes\n",
    "# Files contain all\n",
    "#   Nodes (all user data)\n",
    "#   Edges (just Tweets, Retweets are filtered out in the above steps)\n",
    "\n",
    "nodes_df.drop(columns='author_id', inplace=True)\n",
    "export_nodes_df = clean_nodes(nodes_df)\n",
    "export_edges_df = clean_edges(edges_df)\n",
    "export_nodes_df.to_csv('../Data/Griffin/nodes.csv', index=False)\n",
    "export_edges_df.to_csv('../Data/Griffin/edges.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating basic metrics and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Communication (OUT-Degree) and IN-Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate number of nodes at a specific minimum of filter criterion\n",
    "def calculate_nodes(df:pd.DataFrame, metric:str, min:int):\n",
    "    \n",
    "    nodes_count = len(df[df[metric] >= min])\n",
    "\n",
    "    return nodes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate number of edges at a specific minimum of filter criterion\n",
    "def calculate_edges(df:pd.DataFrame, metric:str, min:int):\n",
    "    \n",
    "    edges_count = int(df[df[metric] >= min].sum())\n",
    "\n",
    "    return edges_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dataframe containing number of nodes and edgs at different levels of filters\n",
    "\n",
    "def calculate_metrics(df, metric):\n",
    "    node_count = []\n",
    "    edge_count = []\n",
    "    min = []\n",
    "\n",
    "    for i in range(1, 1000):\n",
    "        node_count.append(calculate_nodes(df, metric, i))\n",
    "        edge_count.append(calculate_edges(df, metric, i))\n",
    "\n",
    "        min.append(i)\n",
    "\n",
    "    metrics = pd.DataFrame({\n",
    "                        'node_count':node_count,\n",
    "                        'edges_count':edge_count}\n",
    "                        ,index=min)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IN-Degree Metric\n",
    "degree_df = pd.DataFrame(data={'in_degree':edges_df.groupby(by='dst_id_str').count()['src_id_str']})\n",
    "degree_df.sort_values(by='in_degree', ascending=False, inplace=True)\n",
    "degree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_metrics = calculate_metrics(degree_df,'in_degree')\n",
    "deg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(metrics, start, end):\n",
    "    \n",
    "    plt.figure(figsize=(14,4))\n",
    "\n",
    "    x = range(start,end)\n",
    "    y1 = metrics['node_count'].iloc[start:end].values\n",
    "    y2 = metrics['edges_count'].iloc[start:end].values\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(x,y1)\n",
    "    plt.xlabel('min degree')\n",
    "    plt.ylabel('number of nodes')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.plot(x,y2)\n",
    "    plt.xlabel('min degree')\n",
    "    plt.ylabel('number of edges')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(deg_metrics,20,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably a degree around 50 is a reasonable value to filter the nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently there are only 758 nodes with a degree greater than 50, with 204000 Edges connecting them\n",
    "deg_metrics.loc[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_nodes_filtered = degree_df[degree_df['in_degree'] >= 30]\n",
    "len(dst_nodes_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking if the calculated numbers above are correct\n",
    "dst_nodes_filtered.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only edges, where dst_id_str is contained in nodes with degree >= 50\n",
    "edges_deg_filtered_df = edges_df[edges_df['dst_id_str'].isin(dst_nodes_filtered.index)]\n",
    "edges_deg_filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Communication for remaining nodes\n",
    "com_src_nodes = pd.DataFrame(edges_deg_filtered_df.groupby(by=['src_id_str']).count()['src'])\n",
    "com_src_nodes.rename(columns={'src':'communication_freq'}, inplace=True)\n",
    "len(com_src_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_src_nodes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_metrics = calculate_metrics(com_src_nodes, 'communication_freq')\n",
    "com_metrics.loc[3:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all Edges (and SRC Nodes), which contribute less than average to the network (min. 8 Tweets, Replies, Quotes)\n",
    "src_nodes_filtered = com_src_nodes[com_src_nodes['communication_freq'] >= 8]\n",
    "src_nodes_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_filtered_df = edges_deg_filtered_df[edges_deg_filtered_df['src_id_str'].isin(src_nodes_filtered.index)]\n",
    "edges_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select nodes which are SRC or DST \n",
    "nodes_filtered_df = nodes_df[nodes_df['author_id'].isin(dst_nodes_filtered.index) | nodes_df['author_id'].isin(src_nodes_filtered.index)]\n",
    "nodes_filtered_df.reset_index(drop=True, inplace=True)\n",
    "nodes_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nodes(df):\n",
    "    #clean name from unwanted characters\n",
    "    df['name'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "    df['name'] = df['name'].replace({'\"': '' }, regex=True)\n",
    "    df['name'] = df['name'].replace({'\\'': '' }, regex=True)\n",
    "    df['name'] = df['name'].replace({' &amp;': '' }, regex=True)\n",
    "    df['name'] = df['name'].replace({';': ',' }, regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_edges(df):\n",
    "    #clean twitter text form unwanted characters\n",
    "    df['text'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "    df['text'] = df['text'].replace({'\"': '' }, regex=True)\n",
    "    df['text'] = df['text'].replace({';': ',' }, regex=True)\n",
    "    df['text'] = df['text'].replace({' &amp;': '' }, regex=True)\n",
    "    df['text'] = df['text'].replace({'&amp;': '' }, regex=True)\n",
    "    #clean source- and destination-name from unwanted characters\n",
    "    df['src_name'] = df['src_name'].replace({'\\'': '' }, regex=True)\n",
    "    df['dst_name'] = df['dst_name'].replace({'\\'': '' }, regex=True)\n",
    "    df['src_name'] = df['src_name'].replace({';': ',' }, regex=True)\n",
    "    df['dst_name'] = df['dst_name'].replace({';': ',' }, regex=True)\n",
    "    #format the datetime to be griffin readable\n",
    "    df['created_at'] = pd.to_datetime(df[\"created_at\"], format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Edges and Nodes\n",
    "\n",
    "nodes_filtered_df.drop(columns='author_id', inplace=True)\n",
    "export_nodes_df = clean_nodes(nodes_filtered_df)\n",
    "export_edges_df = clean_edges(edges_filtered_df)\n",
    "export_nodes_df.to_csv('nodes_d30_com8.csv', index=False)\n",
    "export_edges_df.to_csv('edges_d30_com8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_18 = export_edges_df[export_edges_df['created_at'].dt.year == 2018] \n",
    "sub_df_19 = export_edges_df[export_edges_df['created_at'].dt.year == 2019]\n",
    "sub_df_20 = export_edges_df[export_edges_df['created_at'].dt.year == 2020] \n",
    "sub_df_21 = export_edges_df[export_edges_df['created_at'].dt.year == 2021] \n",
    "sub_df_22 = export_edges_df[export_edges_df['created_at'].dt.year == 2022]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df_18.to_csv('FEdges18.csv', index=False)\n",
    "sub_df_19.to_csv('FEdges19.csv', index=False)\n",
    "sub_df_20.to_csv('FEdges20.csv', index=False)\n",
    "sub_df_21.to_csv('FEdges21.csv', index=False)\n",
    "sub_df_22.to_csv('FEdges22.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1690ec05146cde089125ff3743f8f3b00aae5d865cc034123d26e20e3a662ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
