{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd662da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages/libraries\n",
    "import datetime\n",
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb4ea1",
   "metadata": {},
   "source": [
    "### Testing Twitter Search API using Tweepy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e980764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the bearer_token for authentification\n",
    "\n",
    "# Essential Access : \n",
    "#bearer_token = 'AAAAAAAAAAAAAAAAAAAAAISGbQEAAAAAiydrn91VCyHyuB1ASiIQofGIGV4%3D8nlmDY77NJPRZQ0uOiGHOllLazV3BXpRfjpunqEdWiXUughu5P'\n",
    "\n",
    "# Research Access :\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAGjtbgEAAAAAyx9oZIai1hXZ9OyDhUUFMZQivpc%3DUunewUXR9hw3nyKQhjqdmfg7zSAoa1nPv6WKLLSPB7OwKwYBP3'\n",
    "\n",
    "# Initializing the client to request the Twitter API\n",
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)\n",
    "\n",
    "query = 'from:Luisamneubauer'\n",
    "#extinctionr #extinctionrebellion\n",
    "\n",
    "#expansions = ['attachments.poll_ids', 'attachments.media_keys', 'author_id', 'entities.mentions.username', 'geo.place_id', 'in_reply_to_user_id', 'referenced_tweets.id', 'referenced_tweets.id.author_id']\n",
    "expansions = ['attachments.media_keys','author_id','geo.place_id','referenced_tweets.id','referenced_tweets.id.author_id']\n",
    "\n",
    "max_results = 500\n",
    "\n",
    "media_fields = [\"duration_ms\", \"height\", \"media_key\", \"preview_image_url\", \"type\", \"url\", \"width\", \"public_metrics\", \"non_public_metrics\", \"organic_metrics\", \"promoted_metrics\", \"alt_text\"]\n",
    "\n",
    "#next_token = ''#(str | None) – This parameter is used to get the next ‘page’ of results. The value used with the parameter is pulled directly from the response provided by the API, and should not be modified. You can learn more by visiting our page on pagination.\n",
    "\n",
    "place_fields = ['country', 'country_code', 'full_name', 'geo', 'id', 'name', 'place_type']\n",
    "\n",
    "#poll_fields #(list[str] | str | None) – poll_fields\n",
    "\n",
    "#since_id #(int | str | None) – Returns results with a Tweet ID greater than (for example, more recent than) the specified ID. The ID specified is exclusive and responses will not include it. If included with the same request as a start_time parameter, only since_id will be used.\n",
    "\n",
    "sort_order = 'recency'\n",
    "\n",
    "start_time = '2017-01-01T00:00:00Z' #(datetime.datetime | str | None) – YYYY-MM-DDTHH:mm:ssZ (ISO 8601/RFC 3339). The oldest UTC timestamp from which the Tweets will be provided. Timestamp is in second granularity and is inclusive (for example, 12:00:01 includes the first second of the minute). By default, a request will return Tweets from up to 30 days ago if you do not include this parameter.\n",
    "#end_time = '2019-07-15T23:00:00Z'\n",
    "\n",
    "#tweet_fields = [\"attachments\", \"author_id\", \"context_annotations\", \"conversation_id\", \"created_at\", \"entities\", \"geo\", \"id\", \"in_reply_to_user_id\", \"lang\", \"non_public_metrics\", \"public_metrics\", \"organic_metrics\", \"promoted_metrics\", \"possibly_sensitive\", \"referenced_tweets\", \"reply_settings\", \"source\", \"text\", \"withheld\"]\n",
    "tweet_fields = [\"attachments\",\"author_id\",\"conversation_id\",\"created_at\",\"referenced_tweets\",\"geo\",\"public_metrics\"]\n",
    "\n",
    "#until_id #(int | str | None) – Returns results with a Tweet ID less than (that is, older than) the specified ID. Used with since_id. The ID specified is exclusive and responses will not include it.\n",
    "\n",
    "# user_fields can be extracted from the .includes attribute of the tweepy response\n",
    "user_fields = [\"created_at\", \"description\", \"location\", \"name\", \"pinned_tweet_id\", \"profile_image_url\", \"protected\", \"public_metrics\", \"url\", \"username\", \"verified\", \"withheld\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7dbb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_recent_tweets():\n",
    "    response = client.search_recent_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=100\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              #,start_time=start_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705a78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_recent_tweets_with_next_token(next_token):\n",
    "    response = client.search_recent_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=100\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              #,start_time=start_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                              ,next_token=next_token\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addc1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_tweets(query):\n",
    "    response = client.search_all_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=max_results\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              ,start_time=start_time\n",
    "                              #,end_time=end_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8323d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_tweets_with_next_token(query, next_token):\n",
    "    response = client.search_all_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=max_results\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              ,start_time=start_time\n",
    "                              #,end_time=end_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                              ,next_token=next_token\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a66768a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_recent_data(iteration_count, next_token):\n",
    "\n",
    "    \"\"\" Fetches iteration_count * 100 Tweets from Twitter API using the set query params, including the initial next_token\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    * iteration_count     : Integer indicating the max number of requests \n",
    "    * next_token          : String containing the next token for pagination\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the return parameters\n",
    "    tweets = []\n",
    "    users = []\n",
    "    ref_tweets = []\n",
    "    media = []\n",
    "    places = []\n",
    "    \n",
    "    for i in range(0, iteration_count):\n",
    "\n",
    "        if(len(next_token) == 0):\n",
    "            response = search_recent_tweets()\n",
    "        else:\n",
    "            response = search_recent_tweets_with_next_token(next_token)\n",
    "\n",
    "        # Parsing data into DataFrames for return \n",
    "        tweets.append(response.data)\n",
    "        users.append(response.includes['users'])\n",
    "        ref_tweets.append(response.includes['tweets'])\n",
    "        if \"media\" in response:\n",
    "            media.append(response.includes['media'])\n",
    "        \n",
    "        if('places' in response.includes.keys()):\n",
    "            places.append(response.includes['places'])\n",
    "\n",
    "        try:\n",
    "            next_token = response.meta['next_token']\n",
    "        except:\n",
    "            print('No more Tweets to fetch')\n",
    "            break\n",
    "\n",
    "\n",
    "    # Logging the last next_token for pagination into console --> needed for later requesting of the next tweets\n",
    "    if('next_token' in response.meta):\n",
    "        print(response.meta['next_token'])\n",
    "        \n",
    "    return tweets, users, ref_tweets, media, places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a597a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_data(query, iteration_count, next_token):\n",
    "\n",
    "    \"\"\" Fetches iteration_count * 100 Tweets from Twitter API using the set query params, including the initial next_token\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    * iteration_count     : Integer indicating the max number of requests \n",
    "    * next_token          : String containing the next token for pagination\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the return parameters\n",
    "    tweets = []\n",
    "    users = []\n",
    "    ref_tweets = []\n",
    "    media = []\n",
    "    places = []\n",
    "    \n",
    "    for i in range(0, iteration_count):\n",
    "\n",
    "        if(len(next_token) == 0):\n",
    "            response = search_all_tweets(query)\n",
    "        else:\n",
    "            response = search_all_tweets_with_next_token(query, next_token)\n",
    "\n",
    "        tweets.append(response.data)\n",
    "        users.append(response.includes['users'])\n",
    "        ref_tweets.append(response.includes['tweets'])\n",
    "        if \"media\" in response.includes.keys():\n",
    "            media.append(response.includes['media'])\n",
    "        \n",
    "        if('places' in response.includes.keys()):\n",
    "            places.append(response.includes['places'])\n",
    "            \n",
    "        try:\n",
    "            next_token = response.meta['next_token']\n",
    "        except:\n",
    "            print('No more Tweets to fetch')\n",
    "            break\n",
    "\n",
    "\n",
    "    # Logging the last next_token for pagination into console --> needed for later requesting of the next tweets\n",
    "    if('next_token' in response.meta):\n",
    "        print(response.meta['next_token'])\n",
    "        \n",
    "    return tweets, users, ref_tweets, media, places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25282fdf-50fa-4a11-8eda-c900b7adcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dict(unpack_dict):\n",
    "    unpacked = [v for k, v in unpack_dict.items()]\n",
    "    return unpacked\n",
    "def unpack_attachment(attachment_dict):\n",
    "    return \"|\".join(attachment_dict[\"media_keys\"]) if \"media_keys\" in attachment_dict.keys() else np.nan\n",
    "def unpack_geo(geo_dict):\n",
    "    return geo_dict[\"type\"], \"|\".join([str(x) for x in geo_dict[\"bbox\"]])\n",
    "def unpack_geo_2(geo_dict):\n",
    "    return geo_dict[\"place_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25f736dc-11cd-4fea-8bc2-2094da8ba281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tweets(query):    \n",
    "    tweets, users, ref_tweets, media, places = fetch_all_data(query, 300, '')\n",
    "\n",
    "    user_df = pd.DataFrame(data=[user for sublist in users for user in sublist])\n",
    "    tweet_df = pd.DataFrame(data=[tweet for sublist in tweets for tweet in sublist])\n",
    "    ref_tweet_df = pd.DataFrame(data=[tweet for sublist in ref_tweets for tweet in sublist])\n",
    "    media_df = pd.DataFrame(data=[item for sublist in media for item in sublist])\n",
    "    place_df = pd.DataFrame(data=[place for sublist in places for place in sublist])\n",
    "    \n",
    "    user_df[[\"followers_count\", \"following_count\", \"tweet_count\", \"listed_count\"]] = user_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    del user_df[\"public_metrics\"]\n",
    "\n",
    "    tweet_df[[\"retweet_count\", \"reply_count\", \"like_count\", \"quote_count\"]] = tweet_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    tweet_df[\"media_keys\"] = tweet_df.loc[tweet_df[\"attachments\"].notna()].apply(lambda x: unpack_attachment(x[\"attachments\"]), axis=1, result_type=\"expand\")\n",
    "    del tweet_df[\"public_metrics\"]\n",
    "    del tweet_df[\"attachments\"]\n",
    "    del tweet_df[\"referenced_tweets\"]\n",
    "\n",
    "    ref_tweet_df[['retweet_count', 'reply_count', 'like_count', 'quote_count']] = ref_tweet_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    ref_tweet_df[\"media_keys\"] = ref_tweet_df.loc[ref_tweet_df[\"attachments\"].notna()].apply(lambda x: unpack_attachment(x[\"attachments\"]), axis=1, result_type=\"expand\")\n",
    "    ref_tweet_df['place_id'] = ref_tweet_df.loc[ref_tweet_df[\"geo\"].notna()].apply(lambda x: unpack_geo_2(x[\"geo\"]), axis=1, result_type=\"expand\")\n",
    "    del ref_tweet_df[\"public_metrics\"]\n",
    "    del ref_tweet_df[\"attachments\"]\n",
    "    del ref_tweet_df[\"geo\"]\n",
    "    del ref_tweet_df[\"referenced_tweets\"]\n",
    "\n",
    "    if \"geo\" in place_df.columns:\n",
    "        place_df[[\"type\", \"coordinates\"]] = place_df.loc[place_df[\"geo\"].notna()].apply(lambda x: unpack_geo(x[\"geo\"]), axis=1, result_type=\"expand\")\n",
    "        del place_df[\"geo\"]\n",
    "        \n",
    "    tweet_df.text = tweet_df.text.apply(lambda x: re.sub(r\"[^A-Za-z0-9\\w\\s:@,]\", \"\", x))\n",
    "    return user_df, tweet_df, ref_tweet_df, place_df, media_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c45763b5-ebae-4c7f-a1e2-313123557cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sa\n",
    "import tweepy\n",
    "import dotenv\n",
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "username = os.environ[\"DB_USERNAME\"]\n",
    "password = os.environ[\"DB_PASSWORD\"]\n",
    "host = os.environ[\"DB_HOST\"]\n",
    "port = os.environ[\"DB_PORT\"]\n",
    "name = os.environ[\"DB_NAME\"]\n",
    "engine = sa.create_engine(\"mssql+pymssql://{}:{}@{}/{}\".format(username, password, host, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674eb2a-b3b2-4576-a327-de07c7e968e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d45a9b56-8ab9-4dc3-a985-c37a98bda182",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from:Fridays4future\n",
      "No more Tweets to fetch\n",
      "from:FridayForFuture\n",
      "No more Tweets to fetch\n",
      "from:GretaThunberg\n",
      "No more Tweets to fetch\n",
      "from:Luisamneubauer\n",
      "No more Tweets to fetch\n",
      "EndCoal\n"
     ]
    },
    {
     "ename": "TwitterServerError",
     "evalue": "503 Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTwitterServerError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-d4a6e4e4f13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     'ClimateChange', 'Climate', 'GlobalWarming']:\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muser_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_tweet_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplace_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mall_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mall_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_user\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-8bf2d20cceeb>\u001b[0m in \u001b[0;36mquery_tweets\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mquery_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0muser_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtweet_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-4dae0a69b06e>\u001b[0m in \u001b[0;36mfetch_all_data\u001b[0;34m(query, iteration_count, next_token)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_all_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_all_tweets_with_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ac4d03dbf63b>\u001b[0m in \u001b[0;36msearch_all_tweets_with_next_token\u001b[0;34m(query, next_token)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearch_all_tweets_with_next_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     response = client.search_all_tweets(\n\u001b[0m\u001b[1;32m      3\u001b[0m                                \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0;34m,\u001b[0m\u001b[0mexpansions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpansions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0;34m,\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/master-program/net02/COIN-Project/venv/lib/python3.9/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36msearch_all_tweets\u001b[0;34m(self, query, **params)\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \"\"\"\n\u001b[1;32m   1144\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         return self._make_request(\n\u001b[0m\u001b[1;32m   1146\u001b[0m             \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/2/tweets/search/all\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             endpoint_parameters=(\n",
      "\u001b[0;32m~/dev/master-program/net02/COIN-Project/venv/lib/python3.9/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrequest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         response = self.request(method, route, params=request_params,\n\u001b[0m\u001b[1;32m    127\u001b[0m                                 json=json, user_auth=user_auth)\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/master-program/net02/COIN-Project/venv/lib/python3.9/site-packages/tweepy/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTooManyRequests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTwitterServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mHTTPException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTwitterServerError\u001b[0m: 503 Service Unavailable"
     ]
    }
   ],
   "source": [
    "all_user = all_tweets = all_ref_tweets = all_places = all_media = pd.DataFrame()\n",
    "for i in ['from:Fridays4future', 'from:FridayForFuture', 'from:GretaThunberg', 'from:Luisamneubauer', 'EndCoal',\n",
    "    'EndFossilFuels', \n",
    "    'PeopleNotProfit', 'NoMoreEmptyPromises', 'UprootTheSystem', 'FridaysForFuture', \n",
    "    'ClimateAction', 'ClimateJustice', 'ClimateEmergency', 'ClimateStrike', 'SaveThePlanet', 'climatescam', \n",
    "    'climatechangehoax', 'fakeclimate', 'climatehoax', 'globalwarmingisahoax', 'ClimateCrisis', \n",
    "    'ClimateChange', 'Climate', 'GlobalWarming']:\n",
    "    print(i)\n",
    "    user_df, tweet_df, ref_tweet_df, place_df, media_df = query_tweets(i)\n",
    "    all_user = pd.concat([all_user, user_df])\n",
    "    all_tweets = pd.concat([all_user, tweet_df])\n",
    "    all_ref_tweets = pd.concat([all_user, ref_tweet_df])\n",
    "    all_places = pd.concat([all_user, place_df])\n",
    "    all_media = pd.concat([all_user, media_df])\n",
    "    \n",
    "    #user_df.created_at = user_df.created_at.astype(str)\n",
    "    #tweet_df.created_at = tweet_df.created_at.astype(str)\n",
    "    #ref_tweet_df.created_at = ref_tweet_df.created_at.astype(str)\n",
    "    #for k, v in {\"users\": user_df, \"tweets\": tweet_df, \"ref_tweets\": ref_tweet_df, \"places\": place_df, \"media\": media_df}.items():\n",
    "    #    v.to_sql(k, con=engine, index=False, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b53d4f",
   "metadata": {},
   "source": [
    "## Export cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv files\n",
    "user_df.to_csv('./data/User_Data_' + query.replace('from:','') + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '.csv', index=False)\n",
    "tweet_df.to_csv('./data/Tweet_Data_' + query.replace('from:','') + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '.csv', index=False)\n",
    "ref_tweet_df.to_csv('./data/Ref_Tweet_Data_' + query.replace('from:','') + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '.csv', index=False)\n",
    "media_df.to_csv('./data/Media_Data_' + query.replace('from:','') + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '.csv', index=False)\n",
    "place_df.to_csv('./data/Place_Data_' + query.replace('from:','') + '_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0bb8b-74cd-4083-be8c-04b32c2dfbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
