{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7961493c",
   "metadata": {},
   "source": [
    "# Twitter API\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0db7a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "import time\n",
    "from datetime import timedelta,timezone\n",
    "from time import sleep\n",
    "from pandas import json_normalize\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "from pandas import DataFrame\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ef3ad",
   "metadata": {},
   "source": [
    "#### Twitter API KEY & Token for the developer account of vithu92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69915ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your bear token here\n",
    "BEAR_TOKEN = \"\"\n",
    "os.environ['TWITTER_TOKEN'] = BEAR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0eebf",
   "metadata": {},
   "source": [
    "#### Functions implemented for fetching data from Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterAPI:\n",
    "    request_counter:int = 0\n",
    "    request_limit:int = 300\n",
    "    def __init__(self):\n",
    "        self.request_counter = 0\n",
    "    def auth(self):\n",
    "        return os.getenv('TWITTER_TOKEN')\n",
    "\n",
    "    def create_headers(self,bearer_token):\n",
    "        headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "        return headers\n",
    "\n",
    "    def create_csv_from_json(self,filename:str, keyword:str):\n",
    "        with open(f'results/json/{filename}.json') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            df = json_normalize(data['data'])\n",
    "        with open(f\"results/json/{filename}_user.json\") as json_file:\n",
    "            user = json.load(json_file)\n",
    "            df_user = json_normalize(user['users'])\n",
    "            df_user.rename(columns={'id':'author_id'}, inplace=True)\n",
    "        df_merge = pd.merge(df, df_user, on='author_id')\n",
    "        df_merge.insert(0, 'Keyword', f'{keyword}')\n",
    "        df_merge.to_csv(f\"results/csv/{filename}.csv\")\n",
    "\n",
    "    def get_tweeter_user(self,user_id:str=None, username:str=None):\n",
    "        headers = self.create_headers(self.auth())\n",
    "        if user_id:\n",
    "            search_url = f\"https://api.twitter.com/2/users/{user_id}\"\n",
    "        if username:\n",
    "            search_url = f\"https://api.twitter.com/2/users/by/username/{username}\"\n",
    "\n",
    "        query_params = {'user.fields': 'public_metrics'}\n",
    "        response = self.connect_to_endpoint(url=search_url, headers=headers, params=query_params, next_token=None)\n",
    "        return response['data']\n",
    "\n",
    "\n",
    "    def create_url(self,keyword:str,griffin:bool,start_date:str, end_date:str,years:int=0,max_results = 500):\n",
    "\n",
    "        search_url = \"https://api.twitter.com/2/tweets/search/all\" #Change to the endpoint you want to collect data from\n",
    "\n",
    "        #change params based on the endpoint you are using\n",
    "        #start_date = datetime.datetime.now(datetime.timezone.utc) - timedelta(days=365*years)\n",
    "        #start_date = start_date.isoformat()\n",
    "        #end_date = datetime.datetime.now(datetime.timezone.utc) - timedelta(days=365*(years-1))\n",
    "        #end_date = end_date.isoformat()\n",
    "        print(start_date)\n",
    "        print(end_date)\n",
    "        if griffin:\n",
    "            start_date = datetime.strptime(start_date, '%d.%m.%Y')\n",
    "            start_date = start_date.astimezone(pytz.UTC)\n",
    "            end_date = datetime.strptime(end_date, '%d.%m.%Y')\n",
    "            end_date = end_date.astimezone(pytz.UTC)\n",
    "            query_params = {'query': keyword,\n",
    "                           'start_time': start_date.isoformat(),\n",
    "                            'end_time': end_date.isoformat(),\n",
    "                            'tweet.fields':'author_id,created_at,in_reply_to_user_id,public_metrics,entities',\n",
    "                            'expansions': 'attachments.media_keys,author_id,in_reply_to_user_id,referenced_tweets.id,entities.mentions.username,referenced_tweets.id.author_id',\n",
    "                            'user.fields': 'id,name,public_metrics,username',\n",
    "                            'media.fields':'url',\n",
    "                            'max_results': max_results,\n",
    "                            'sort_order':\"recency\",\n",
    "                            'next_token':{}}\n",
    "        else:\n",
    "            query_params = {'query': keyword,\n",
    "                            'start_time': start_date,\n",
    "                            #'end_time': end_date,\n",
    "                            'max_results': max_results,\n",
    "                            'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,geo.place_id,in_reply_to_user_id,referenced_tweets.id,entities.mentions.username,referenced_tweets.id.author_id',\n",
    "                            'tweet.fields': \"attachments,created_at,author_id\",\n",
    "                            'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "                            'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "                            'media.fields':'duration_ms,height,media_key,preview_image_url,public_metrics,type,url,width',\n",
    "                            'next_token': {}}\n",
    "        return (search_url, query_params)\n",
    "\n",
    "\n",
    "    def connect_to_endpoint(self,url, headers, params, next_token = None):\n",
    "        params['next_token'] = next_token   #params object received from create_url function\n",
    "        retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        http = requests.Session()\n",
    "        http.mount(\"https://\", adapter)\n",
    "        http.mount(\"http://\", adapter)\n",
    "        \n",
    "        response = http.get(url=url, headers = headers, params = params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(response.status_code, response.text)        \n",
    "        sleep(2)\n",
    "        return response.json()\n",
    "\n",
    "    def get_date_range(self,delta:int=7):\n",
    "        end_date = datetime.datetime.now(datetime.timezone.utc)\n",
    "        end_date = end_date - timedelta(hours=1)\n",
    "        start_date = end_date - timedelta(days=delta)\n",
    "        start_date = start_date + timedelta(hours=1)\n",
    "        return start_date.isoformat(), end_date.isoformat()\n",
    "\n",
    "    def cast_dict(self,x, name):\n",
    "        d = {}\n",
    "        if isinstance(x,list):\n",
    "            for k,v in x[0].items():\n",
    "                 d[\"{}_{}\".format(name, k)] = v\n",
    "        return pd.Series(d)\n",
    "    \n",
    "    def get_user_dataframe(self,data:dict):\n",
    "        row_list = []\n",
    "    \n",
    "        for user in data['includes']['users']:\n",
    "            user_dict={}\n",
    "            user_dict['favourites_count'] = 0\n",
    "            user_dict['followers_count'] = user.get(\"public_metrics\",{}).get('followers_count',\"\")\n",
    "            user_dict['friends_count'] =  user.get(\"public_metrics\",{}).get('following_count',\"\")\n",
    "            user_dict['listed_count']= user.get(\"public_metrics\",{}).get('listed_count',\"\")\n",
    "            user_dict['statuses_count'] = user.get(\"public_metrics\",{}).get('tweet_count',\"\")\n",
    "            user_dict['name'] = user.get('name',\"\")\n",
    "            user_dict['screen_name'] = user.get('username',\"\")\n",
    "            user_dict['id_str'] =  user.get('id',\"\")\n",
    "\n",
    "            row_list.append(user_dict)\n",
    "        df = pd.DataFrame(row_list)\n",
    "        df_media_outlets = pd.read_csv(\"media_outlets.csv\")\n",
    "        media_list = df_media_outlets['@name'].apply(lambda x: x[1:]).tolist()\n",
    "        for index, row in df.iterrows():\n",
    "            if row['screen_name'] in media_list:\n",
    "                df.at[index,'media_outlet'] = True\n",
    "            else:\n",
    "                df.at[index, 'media_outlet'] = False\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def get_media_dataframe(self, data:dict):\n",
    "        row_list =[]\n",
    "        if len(data.get('includes','').get('media','')) == 0:\n",
    "            return None\n",
    "        for media in data['includes']['media']:\n",
    "            media_dict = {}\n",
    "            #['media_key', 'type', 'url']\n",
    "            media_dict['media_key'] = media.get('media_key','')\n",
    "            media_dict['type'] = media.get('type','')\n",
    "            media_dict['url'] = media.get('url','')\n",
    "            row_list.append(media_dict)\n",
    "        \n",
    "        return pd.DataFrame(row_list)\n",
    "\n",
    "\n",
    "    def get_tweet_dataframe(self,data:dict):\n",
    "        row_list = []\n",
    "        for entry in data['data']:\n",
    "            data_dict = {}\n",
    "            data_dict['text'] = entry.get('text',\"\")\n",
    "            data_dict['created_at'] = entry.get('created_at',\"\")\n",
    "            data_dict['id_str'] = entry.get('author_id',\"\")\n",
    "            # hashtags\n",
    "            hashtags_list = entry.get('entities',{}).get(\"hashtags\",[])\n",
    "            if len(hashtags_list) != 0:\n",
    "                data_dict['hashtags'] = hashtags_list[0].get('tag','')\n",
    "\n",
    "            else:\n",
    "                data_dict['hashtags'] = \"\"\n",
    "            # destination user\n",
    "            mentions_list = entry.get('entities',{}).get(\"mentions\",[])\n",
    "            if len(mentions_list) != 0:\n",
    "                data_dict['dst_id_str'] = mentions_list[0].get('id',\"\")\n",
    "            else:\n",
    "                data_dict['dst_id_str'] = \"\"\n",
    "            media_list = entry.get('attachments',{}).get('media_keys',[])\n",
    "            if len(media_list) != 0:\n",
    "                data_dict['media_keys'] = media_list\n",
    "            else:\n",
    "                data_dict['media_keys'] = \"\"\n",
    "            row_list.append(data_dict)\n",
    "            \n",
    "        return pd.DataFrame(row_list)\n",
    "\n",
    "    def add_prefix_to_user_columns(self,df:DataFrame, prefix:str):\n",
    "        column_list = ['favourites_count', 'followers_count', 'friends_count', 'listed_count',\n",
    "           'statuses_count', 'name', 'screen_name', 'id_str']\n",
    "        new_names = [(i,f\"{prefix}_\"+i) for i in column_list]\n",
    "        df_renamed = df.rename(columns = dict(new_names))\n",
    "\n",
    "        return df_renamed\n",
    "\n",
    "    def get_data_dataframe(self,data:dict):\n",
    "        df_user = self.get_user_dataframe(data=data)\n",
    "        df_tweet= self.get_tweet_dataframe(data=data)\n",
    "        # merging src user\n",
    "        df_total = pd.merge(left=df_tweet, right=df_user, how=\"left\", on=['id_str'])\n",
    "        df_total = self.add_prefix_to_user_columns(df_total, prefix=\"src\")\n",
    "        # mergin dst \n",
    "        df_total = df_total.rename(columns={\"dst_id_str\": \"id_str\"})\n",
    "        df_user_dest = df_user[['name', 'screen_name', 'id_str']]\n",
    "        df_total = pd.merge(left=df_total, right=df_user_dest, how=\"left\", on=['id_str'])\n",
    "        df_total = self.add_prefix_to_user_columns(df_total, prefix=\"dst\")\n",
    "    \n",
    "        # adding custom columns\n",
    "        df_total['src'] = df_total['src_screen_name'].str.lower()\n",
    "        df_total['dst'] = df_total['dst_screen_name'].str.lower()\n",
    "        \n",
    "        return df_total, df_user\n",
    "\n",
    "    def get_tweets_griffin(self,keyword:str, start_date:str, end_date:str,years:int=0,media:bool=False,limit:int=None):\n",
    "        headers = self.create_headers(self.auth())\n",
    "        search_url, params = self.create_url(keyword=keyword, years=years, griffin=True,start_date=start_date, end_date=end_date,max_results=500)\n",
    "        json_response = self.connect_to_endpoint(url=search_url, headers=headers, params=params)\n",
    "        df,df_user = self.get_data_dataframe(data=json_response)\n",
    "        if media:\n",
    "            df_media = self.get_media_dataframe(data=json_response)\n",
    "        tweets_counter = json_response['meta']['result_count']\n",
    "        print(f\"Fetched {tweets_counter} tweets\",end='\\r')\n",
    "        while 'next_token' in json_response['meta']:\n",
    "            try:\n",
    "                json_response = self.connect_to_endpoint(url=search_url, headers=headers, params=params, next_token=str(json_response['meta']['next_token']))\n",
    "            except:\n",
    "                break\n",
    "            \n",
    "            if isinstance(df, pd.DataFrame):\n",
    "                df_tmp, df_user_tmp = self.get_data_dataframe(data=json_response)\n",
    "                df = df.append(df_tmp)\n",
    "                df_user = df_user.append(df_user_tmp)\n",
    "            \n",
    "            if media:\n",
    "                if isinstance(df_media, pd.DataFrame):\n",
    "                    if len(json_response.get('includes','').get('media','')) != 0:\n",
    "                        df_media = df_media.append(self.get_media_dataframe(data=json_response))\n",
    "            tweets_counter += json_response['meta']['result_count']\n",
    "            print(f\"Fetched {tweets_counter} tweets\",end='\\r')\n",
    "            if limit:\n",
    "                if tweets_counter >= limit:\n",
    "                    break\n",
    "            \n",
    "        \n",
    "        # add keyword as column\n",
    "        df['twitter_query'] = keyword\n",
    "        \n",
    "        # timestamp\n",
    "        df['time'] = df['created_at'].apply(lambda x: (int((parser.parse(x)).timestamp())))\n",
    "        \n",
    "        #  rearranging\n",
    "        #df = df.reindex(columns=['src_favourites_count', 'src_followers_count', 'src_friends_count',\n",
    "       #'src_listed_count', 'src_statuses_count', 'src_name', 'src_screen_name',\n",
    "       #'src_id_str', 'text', 'created_at', 'twitter_query', 'hashtags',\n",
    "       #'dst_name', 'dst_screen_name', 'dst_id_str', 'time', 'src', 'dst'])\n",
    "        \n",
    "        # cleaning\n",
    "        df = self.clean_edges(df)\n",
    "        df_user = self.clean_nodes(df_user)\n",
    "        \n",
    "        if media:\n",
    "            for index, row in df.iterrows():\n",
    "                if row['media_keys']:\n",
    "                    for entry in row['media_keys']:\n",
    "                        index = df_media.index[df_media['media_key'].str.contains(entry)].tolist()[0]\n",
    "                        df_media.at[index,'src'] = row['src']\n",
    "                        df_media.at[index,'src_id_str'] = row['src_id_str']\n",
    "            \n",
    "            df = df.drop(\"media_keys\",axis=1)\n",
    "            return df, df_user, df_media\n",
    "        else:\n",
    "            return df, df_user\n",
    "    \n",
    "    def clean_edges(self, df):\n",
    "        #clean twitter text form unwanted characters\n",
    "        df['text'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "        df['text'] = df['text'].replace({'\"': '' }, regex=True)\n",
    "        df['text'] = df['text'].replace({';': ',' }, regex=True)\n",
    "        df['text'] = df['text'].replace({' &amp;': '' }, regex=True)\n",
    "        df['text'] = df['text'].replace({'&amp;': '' }, regex=True)\n",
    "        #clean source- and destination-name from unwanted characters\n",
    "        df['src_name'] = df['src_name'].replace({'\\'': '' }, regex=True)\n",
    "        df['dst_name'] = df['dst_name'].replace({'\\'': '' }, regex=True)\n",
    "        df['src_name'] = df['src_name'].replace({';': ',' }, regex=True)\n",
    "        df['dst_name'] = df['dst_name'].replace({';': ',' }, regex=True)\n",
    "        #format the datetime to be griffin readable\n",
    "        df['created_at'] = pd.to_datetime(df[\"created_at\"], format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "        return df\n",
    "\n",
    "    def clean_nodes(self, df):\n",
    "        #clean name from unwanted characters\n",
    "        df['name'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "        df['name'] = df['name'].replace({'\"': '' }, regex=True)\n",
    "        df['name'] = df['name'].replace({'\\'': '' }, regex=True)\n",
    "        df['name'] = df['name'].replace({' &amp;': '' }, regex=True)\n",
    "        df['name'] = df['name'].replace({';': ',' }, regex=True)\n",
    "        return df\n",
    "\n",
    "    def get_tweets(self,keyword:str, filename:str, years:int = 10,griffin:bool=False,limit:int=None):\n",
    "        headers = self.create_headers(auth())\n",
    "        start, end = self.get_date_range()\n",
    "        search_url, params = self.create_url(keyword=keyword, years=years, griffin=griffin)\n",
    "        df_twitter = pd.DataFrame()\n",
    "\n",
    "\n",
    "        json_response = self.connect_to_endpoint(url=search_url, headers=headers, params=params)\n",
    "        counter = 0\n",
    "        tweets_counter = 0\n",
    "        tweets_counter += json_response['meta']['result_count']\n",
    "        length_df = 0\n",
    "        if not 'data' in json_response:\n",
    "            print(f\"No results found for keyword: {keyword}\")\n",
    "            return\n",
    "        try:\n",
    "            while json_response['meta']['next_token']:\n",
    "                #search_url, params = create_url(keyword=keyword,years=years)\n",
    "                json_response = self.connect_to_endpoint(url=search_url, headers=headers, params=params, next_token=str(json_response['meta']['next_token']))\n",
    "\n",
    "                #json_file_data.writelines(\n",
    "                #    \",\" + json.dumps(json_response[\"data\"])[1:-1])\n",
    "                #json_file_user.writelines(\",\" + json.dumps(json_response['includes']['users'])[1:-1])\n",
    "\n",
    "                #normalize tweets and change type to datetime\n",
    "                tweets = pd.json_normalize(json_response['data'])\n",
    "                length_df += len(tweets)\n",
    "                print('number of new rows: ' +  str(len(tweets)))\n",
    "                print('expected number of rows in total: ' + str(length_df))\n",
    "                tweets['created_at'] = pd.to_datetime(tweets['created_at']).dt.strftime('%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "                #cast nested dictionaries to seperate columns with a prefix\n",
    "                tweets = tweets.join(tweets['referenced_tweets'].apply(cast_dict, name = 'referenced_tweets'), how = 'outer')\\\n",
    "                        .drop(\"referenced_tweets\", axis=1)\n",
    "                tweets = tweets.join(tweets['entities.mentions'].apply(cast_dict, name = 'entities.mentions'), how = 'outer')\\\n",
    "                        .drop(\"entities.mentions\", axis=1)\n",
    "                #cast entry in list to column (as it is needed as a key to join media)\n",
    "                tweets['attachments.media_keys'] = tweets['attachments.media_keys'].apply(lambda x: x[0] if isinstance(x,list) else None)\n",
    "                #add a prefix tweet\n",
    "                tweets = tweets.add_prefix('tweet.')\n",
    "\n",
    "                #normalize media and add a prefix to columnnames\n",
    "                media = pd.json_normalize(json_response['includes']['media'])\n",
    "                media = media.add_prefix('media.')\n",
    "\n",
    "                #normalize users, change type to datetime and add a prefix to columnnames\n",
    "                users = pd.json_normalize(json_response['includes']['users'])\n",
    "                users['created_at'] = pd.to_datetime(users['created_at']).dt.strftime('%d.%m.%Y %H:%M:%S')\n",
    "                users = users.add_prefix('user.')\n",
    "\n",
    "                #merge both media and users with the tweets to get all the information in one dataframe\n",
    "                df_total = pd.merge(tweets, media, left_on = 'tweet.attachments.media_keys', right_on = 'media.media_key', how=\"left\")\n",
    "                df_total = pd.merge(df_total, users, left_on = 'tweet.author_id', right_on = 'user.id', how=\"left\")\n",
    "                df_twitter = df_twitter.append(df_total, ignore_index = True)\n",
    "                print('actual number of rows in total: ' + str(len(df_twitter)))\n",
    "                print(tweets_counter)\n",
    "\n",
    "                counter += 1\n",
    "                tweets_counter += json_response[\"meta\"][\"result_count\"]\n",
    "                if limit:\n",
    "                    if tweets_counter > limit:\n",
    "                        break\n",
    "        except:\n",
    "            print(\"no next token\")\n",
    "            df_twitter.to_json(f\"results/json/{filename}.json\")\n",
    "            df_twitter.to_csv(f\"results/csv/{filename}.csv\")\n",
    "            return\n",
    "    \n",
    "    #create_csv_from_json(filename=filename, keyword=keyword)\n",
    "    #df_twitter.to_json(f\"results/json/{keyword}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bed724",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ed190",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_api = TwitterAPI()\n",
    "\n",
    "# the keyword is the search term in twitter api - \n",
    "# explaination for this search team: look for all tweets in english with the substring/hashtag sunrisemvmt\n",
    "# start_date = setup the start date - i did it because otherwise it took my more than 24hourse to fetch all data\n",
    "# end_date => same the format is dd.mm.yyyy\n",
    "\n",
    "# media = Boolean to tell if you need a seperate csv with all the media files (url, user_id)\n",
    "\n",
    "#output: 2 datafraeme:\n",
    "# 1. dataframe = df (tweeter data) => this is going to be the edges\n",
    "# 2. dataframe = user data => this is going to be the nodes\n",
    "df,df_user, df_media = twitter_api.get_tweets_griffin(keyword=\"lang:en (sunrisemvmt)\", start_date=\"17.05.2021\",end_date=\"16.05.2022\", media=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8515f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv\n",
    "df_user.to_csv(\"hashtags_sunrisemvmt_nodes.csv\", index=False)\n",
    "df.to_csv(\"hashtags_sunrisemvmt_edges.csv\", index=False)\n",
    "df_media.to_csv(\"hashtags_sunrisemvmt_media.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ece53-e299-422d-a0dc-683c28f8640a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
