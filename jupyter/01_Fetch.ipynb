{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COIN Project\n",
    "\n",
    "Authors:\n",
    "\n",
    "    - Mona\n",
    "    - Roman\n",
    "    - Nick\n",
    "    - Mateo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy as sa\n",
    "import tweepy\n",
    "#import dotenv\n",
    "import os\n",
    "import yaml\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import time\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../twitter.yml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accounts': ['Fridays4future',\n",
       "  'FridayForFuture',\n",
       "  'GretaThunberg',\n",
       "  'Luisamneubauer',\n",
       "  'AnnaUKSCN',\n",
       "  'AnunaDe',\n",
       "  'adelaidecharli2',\n",
       "  'HollyWildChild',\n",
       "  'AlexandriaV2005',\n",
       "  'fff_europe',\n",
       "  'fff_digital',\n",
       "  'FFF_Sweden',\n",
       "  'FFF_Scotland',\n",
       "  'FFF_Berlin',\n",
       "  'FFFireland',\n",
       "  'fff_hamburg'],\n",
       " 'hashtags': {'green': ['EndCoal',\n",
       "   'EndFossilFuels',\n",
       "   'PeopleNotProfit',\n",
       "   'NoMoreEmptyPromises',\n",
       "   'UprootTheSystem',\n",
       "   'FridaysForFuture',\n",
       "   'ClimateAction',\n",
       "   'ClimateJustice',\n",
       "   'ClimateEmergency',\n",
       "   'ClimateStrike',\n",
       "   'SaveThePlanet'],\n",
       "  'brown': ['climatescam',\n",
       "   'climatechangehoax',\n",
       "   'fakeclimate',\n",
       "   'climatehoax',\n",
       "   'globalwarmingisahoax'],\n",
       "  'neutral': ['ClimateCrisis', 'ClimateChange', 'Climate', 'GlobalWarming']}}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Research Access:\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAGjtbgEAAAAAyx9oZIai1hXZ9OyDhUUFMZQivpc%3DUunewUXR9hw3nyKQhjqdmfg7zSAoa1nPv6WKLLSPB7OwKwYBP3'\n",
    "\n",
    "#Initializing the client to request the Twitter API\n",
    "client = tweepy.Client(bearer_token, wait_on_rate_limit=True)\n",
    "\n",
    "#expansions = ['attachments.poll_ids', 'attachments.media_keys', 'author_id', 'entities.mentions.username', 'geo.place_id', 'in_reply_to_user_id', 'referenced_tweets.id', 'referenced_tweets.id.author_id']\n",
    "expansions = ['attachments.media_keys','author_id','geo.place_id','referenced_tweets.id', 'entities.mentions.username','referenced_tweets.id.author_id']\n",
    "max_results = 100\n",
    "media_fields = [\"duration_ms\", \"height\", \"media_key\", \"preview_image_url\", \"type\", \"url\", \"width\", \"public_metrics\", \"non_public_metrics\", \"organic_metrics\", \"promoted_metrics\", \"alt_text\"]\n",
    "#next_token = ''#(str | None) – This parameter is used to get the next ‘page’ of results. The value used with the parameter is pulled directly from the response provided by the API, and should not be modified. You can learn more by visiting our page on pagination.\n",
    "place_fields = ['country', 'country_code', 'full_name', 'geo', 'id', 'name', 'place_type']\n",
    "#poll_fields #(list[str] | str | None) – poll_fields\n",
    "#since_id #(int | str | None) – Returns results with a Tweet ID greater than (for example, more recent than) the specified ID. The ID specified is exclusive and responses will not include it. If included with the same request as a start_time parameter, only since_id will be used.\n",
    "sort_order = 'recency'\n",
    "start_time = '2017-01-01T00:00:00Z' #(datetime.datetime | str | None) – YYYY-MM-DDTHH:mm:ssZ (ISO 8601/RFC 3339). The oldest UTC timestamp from which the Tweets will be provided. Timestamp is in second granularity and is inclusive (for example, 12:00:01 includes the first second of the minute). By default, a request will return Tweets from up to 30 days ago if you do not include this parameter.\n",
    "#end_time = '2019-07-15T23:00:00Z'\n",
    "#tweet_fields = [\"attachments\", \"author_id\", \"context_annotations\", \"conversation_id\", \"created_at\", \"entities\", \"geo\", \"id\", \"in_reply_to_user_id\", \"lang\", \"non_public_metrics\", \"public_metrics\", \"organic_metrics\", \"promoted_metrics\", \"possibly_sensitive\", \"referenced_tweets\", \"reply_settings\", \"source\", \"text\", \"withheld\"]\n",
    "tweet_fields = [\"attachments\",\"author_id\", \"entities\", \"conversation_id\",\"created_at\",\"referenced_tweets\",\"geo\",\"public_metrics\"]\n",
    "#until_id #(int | str | None) – Returns results with a Tweet ID less than (that is, older than) the specified ID. Used with since_id. The ID specified is exclusive and responses will not include it.\n",
    "#user_fields can be extracted from the .includes attribute of the tweepy response\n",
    "user_fields = [\"created_at\", \"description\", \"location\", \"name\", \"pinned_tweet_id\", \"profile_image_url\", \"protected\", \"public_metrics\", \"url\", \"username\", \"verified\", \"withheld\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Load and Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify tweets to look up\n",
    "\n",
    "#query_list = ['from:Luisamneubauer -is:retweet']\n",
    "\n",
    "#query_list = ['FridaysForFuture -is:retweet']\n",
    "\n",
    "\n",
    "''''query_list = ['from:Fridays4future -is:retweet',\n",
    "              'from:FridayForFuture -is:retweet',\n",
    "              'from:GretaThunberg -is:retweet',\n",
    "              'from:Luisamneubauer -is:retweet',\n",
    "              'from:AnnaUKSCN -is:retweet',\n",
    "              'from:AnunaDe -is:retweet',\n",
    "              'from:adelaidecharli2 -is:retweet',\n",
    "              'from:HollyWildChild -is:retweet',\n",
    "              'from:AlexandriaV2005 -is:retweet',\n",
    "              'from:fff_europe -is:retweet',\n",
    "              'from:fff_digital -is:retweet',\n",
    "              'from:FFF_Sweden -is:retweet',\n",
    "              'from:FFF_Scotland -is:retweet',\n",
    "              'from:FFF_Berlin -is:retweet',\n",
    "              'from:FFFireland -is:retweet',\n",
    "              'from:fff_hamburg -is:retweet'''''\n",
    "\n",
    "'''query_list = ['FridaysForFuture -is:retweet',\n",
    "              'SchoolStrike4Climate -is:retweet',\n",
    "              'fridays4future -is:retweet']'''\n",
    "\n",
    "\n",
    "query_list = ['fridays4future -is:retweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_tweets(query):\n",
    "    response = client.search_all_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=max_results\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              ,start_time=start_time\n",
    "                              #,end_time=end_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_tweets_with_next_token(query, next_token):\n",
    "    response = client.search_all_tweets(\n",
    "                               query=query\n",
    "                              ,expansions=expansions\n",
    "                              ,max_results=max_results\n",
    "                              ,media_fields=media_fields\n",
    "                              ,place_fields=place_fields\n",
    "                              ,sort_order=sort_order\n",
    "                              ,start_time=start_time\n",
    "                              #,end_time=end_time\n",
    "                              ,tweet_fields=tweet_fields\n",
    "                              ,user_fields=user_fields\n",
    "                              ,next_token=next_token\n",
    "                            )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_data(query, iteration_count, next_token):\n",
    "\n",
    "    \"\"\" Fetches iteration_count * 100 Tweets from Twitter API using the set query params, including the initial next_token\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    * iteration_count     : Integer indicating the max number of requests \n",
    "    * next_token          : String containing the next token for pagination\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initializing the return parameters\n",
    "    tweets = []\n",
    "    users = []\n",
    "    ref_tweets = []\n",
    "    media = []\n",
    "    places = []\n",
    "    \n",
    "    for i in range(0, iteration_count):\n",
    "\n",
    "        if(len(next_token) == 0):\n",
    "            response = search_all_tweets(query)\n",
    "        else:\n",
    "            response = search_all_tweets_with_next_token(query, next_token)\n",
    "\n",
    "        tweets.append(response.data)\n",
    "        users.append(response.includes['users'])\n",
    "        ref_tweets.append(response.includes['tweets'])\n",
    "        if \"media\" in response.includes.keys():\n",
    "            media.append(response.includes['media'])\n",
    "        \n",
    "        if('places' in response.includes.keys()):\n",
    "            places.append(response.includes['places'])\n",
    "            \n",
    "        try:\n",
    "            next_token = response.meta['next_token']\n",
    "        except:\n",
    "            print('No more Tweets to fetch')\n",
    "            break\n",
    "\n",
    "\n",
    "    # Logging the last next_token for pagination into console --> needed for later requesting of the next tweets\n",
    "    if('next_token' in response.meta):\n",
    "        print(response.meta['next_token'])\n",
    "        \n",
    "    return tweets, users, ref_tweets, media, places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dict(unpack_dict):\n",
    "    unpacked = [v for k, v in unpack_dict.items()]\n",
    "    return unpacked\n",
    "def unpack_attachment(attachment_dict):\n",
    "    return \"|\".join(attachment_dict[\"media_keys\"]) if \"media_keys\" in attachment_dict.keys() else np.nan\n",
    "def unpack_geo(geo_dict):\n",
    "    return geo_dict[\"type\"], \"|\".join([str(x) for x in geo_dict[\"bbox\"]])\n",
    "def unpack_geo_2(geo_dict):\n",
    "    return geo_dict[\"place_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tweets(query):    \n",
    "    tweets, users, ref_tweets, media, places = fetch_all_data(query, 100000, '')\n",
    "\n",
    "    user_df = pd.DataFrame(data=[user for sublist in users for user in sublist])\n",
    "    tweet_df = pd.DataFrame(data=[tweet for sublist in tweets for tweet in sublist])\n",
    "    ref_tweet_df = pd.DataFrame(data=[tweet for sublist in ref_tweets for tweet in sublist])\n",
    "    media_df = pd.DataFrame(data=[item for sublist in media for item in sublist])\n",
    "    place_df = pd.DataFrame(data=[place for sublist in places for place in sublist])\n",
    "    \n",
    "    user_df[[\"followers_count\", \"following_count\", \"tweet_count\", \"listed_count\"]] = user_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    del user_df[\"public_metrics\"]\n",
    "\n",
    "    tweet_df[[\"retweet_count\", \"reply_count\", \"like_count\", \"quote_count\"]] = tweet_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    tweet_df[\"media_keys\"] = tweet_df.loc[tweet_df[\"attachments\"].notna()].apply(lambda x: unpack_attachment(x[\"attachments\"]), axis=1, result_type=\"expand\")\n",
    "    del tweet_df[\"public_metrics\"]\n",
    "    del tweet_df[\"attachments\"]\n",
    "    del tweet_df[\"referenced_tweets\"]\n",
    "\n",
    "    ref_tweet_df[['retweet_count', 'reply_count', 'like_count', 'quote_count']] = ref_tweet_df.apply(lambda x: unpack_dict(x[\"public_metrics\"]), axis=1, result_type=\"expand\")\n",
    "    ref_tweet_df[\"media_keys\"] = ref_tweet_df.loc[ref_tweet_df[\"attachments\"].notna()].apply(lambda x: unpack_attachment(x[\"attachments\"]), axis=1, result_type=\"expand\")\n",
    "    #ref_tweet_df['place_id'] = ref_tweet_df.loc[ref_tweet_df[\"geo\"].notna()].apply(lambda x: unpack_geo_2(x[\"geo\"]), axis=1, result_type=\"expand\")\n",
    "    del ref_tweet_df[\"public_metrics\"]\n",
    "    del ref_tweet_df[\"attachments\"]\n",
    "    #del ref_tweet_df[\"geo\"]\n",
    "    del ref_tweet_df[\"referenced_tweets\"]\n",
    "    \n",
    "    '''if \"geo\" in place_df.columns:\n",
    "        place_df[[\"type\", \"coordinates\"]] = place_df.loc[place_df[\"geo\"].notna()].apply(lambda x: unpack_geo(x[\"geo\"]), axis=1, result_type=\"expand\")\n",
    "        del place_df[\"geo\"]'''\n",
    "        \n",
    "    tweet_df.text = tweet_df.text.apply(lambda x: re.sub(r\"[^A-Za-z0-9\\w\\s:@,]\", \"\", x))\n",
    "    return user_df, tweet_df, ref_tweet_df, place_df, media_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fridays4future -is:retweet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 414 seconds.\n",
      "Rate limit exceeded. Sleeping for 413 seconds.\n",
      "Rate limit exceeded. Sleeping for 543 seconds.\n",
      "Rate limit exceeded. Sleeping for 413 seconds.\n",
      "Rate limit exceeded. Sleeping for 430 seconds.\n",
      "Rate limit exceeded. Sleeping for 432 seconds.\n",
      "Rate limit exceeded. Sleeping for 505 seconds.\n",
      "Rate limit exceeded. Sleeping for 508 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more Tweets to fetch\n"
     ]
    }
   ],
   "source": [
    "all_user = all_tweets = all_ref_tweets = all_places = all_media = pd.DataFrame()\n",
    "for i in query_list:\n",
    "    print(i)\n",
    "    \n",
    "    try:\n",
    "            user_df, tweet_df, ref_tweet_df, place_df, media_df = query_tweets(i)\n",
    "            all_user = pd.concat([all_user, user_df])\n",
    "            all_tweets = pd.concat([all_tweets, tweet_df])\n",
    "            all_ref_tweets = pd.concat([all_ref_tweets, ref_tweet_df])\n",
    "            all_places = pd.concat([all_places, place_df])\n",
    "            all_media = pd.concat([all_media, media_df])\n",
    "    except:\n",
    "            print('Return')\n",
    "            pass\n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250165, 13)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250165, 14)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add usernames to all_tweets dataframe\n",
    "all_user.rename(columns={\"id\": \"author_id\"}, inplace=True)\n",
    "all_tweets = pd.merge(all_tweets,all_user[['author_id','username']],on='author_id', how='inner')\n",
    "all_tweets = all_tweets.drop_duplicates('id')\n",
    "all_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1: Save data as separate files\n",
    "\n",
    "# save dataframe to pickle\n",
    "all_user.to_pickle(\"users.pkl\")\n",
    "all_tweets.to_pickle(\"tweets.pkl\")\n",
    "all_ref_tweets.to_pickle(\"reftweets.pkl\")\n",
    "all_places.to_pickle(\"places.pkl\")\n",
    "all_media.to_pickle(\"media.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets fetched (rows, columns): (250165, 14)\n",
      "Number of users, both authors AND mentioned users (rows, columns): (630456, 16)\n",
      "Number of ref_tweets (rows, columns): (120371, 13)\n",
      "Number of places, tagged by users in tweets (rows, columns): (7656, 7)\n",
      "Number of media in tweets (rows, columns): (67220, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Number of tweets fetched (rows, columns):', all_tweets.shape)\n",
    "print('Number of users, both authors AND mentioned users (rows, columns):', all_user.shape)\n",
    "print('Number of ref_tweets (rows, columns):', all_ref_tweets.shape)\n",
    "print('Number of places, tagged by users in tweets (rows, columns):', all_places.shape)\n",
    "print('Number of media in tweets (rows, columns):', all_media.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---user---#\n",
    "#authors and mentioned users\n",
    "\n",
    "#---tweets---#\n",
    "#tweets that are either from a specified account or include a specfified hashtag\n",
    "\n",
    "#---ref_tweets---#\n",
    "#A list of Tweets this Tweet refers to. For example, if the parent Tweet is a Retweet,\n",
    "#a Retweet with comment (also known as Quoted Tweet) or a Reply, it will include the related\n",
    "#Tweet referenced to by its parent.\n",
    "\n",
    "#---places---#\n",
    "#When including the expansions=geo.place_id parameter, this includes a list of referenced places\n",
    "#in Tweets, if tagged by a user\n",
    "\n",
    "#---media---#\n",
    "#When including the expansions=attachments.media_keys parameter, this includes a list of images,\n",
    "#videos, and GIFs included in Tweets in the form of media objects "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd10460dd3d146ddcd00478931b69fe3eeab4952c1785708c7385e1d4193c119"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
